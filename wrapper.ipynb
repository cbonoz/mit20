{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import random\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from imblearn.datasets import make_imbalance\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "MAX_ROWS = 10000# None # 100000\n",
    "VISUALIZE = False\n",
    "RANDOM_STATE = 42\n",
    "DOWNSAMPLE=False\n",
    "print('ready')\n",
    "\n",
    "\n",
    "# https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
    "# https://imbalanced-learn.org/stable/api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_readable(label):\n",
    "    return re.sub(\"([a-z])([A-Z])\",\"\\g<1> \\g<2>\",label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://elitedatascience.com/imbalanced-classes\n",
    "def downsample(X_train, y_train):\n",
    "    if not DOWNSAMPLE:\n",
    "        return X_train, y_train\n",
    "    X_train['target'] = y_train\n",
    "    df = X_train\n",
    "    print('old shape', X_train.shape)\n",
    "    # Separate majority and minority classes\n",
    "    df_majority = df[df.target==0]\n",
    "    df_minority = df[df.target!=0]\n",
    "\n",
    "    # Downsample majority class\n",
    "    df_majority_downsampled = resample(df_majority, \n",
    "                                     replace=False,    # sample without replacement\n",
    "                                     n_samples=len(df_minority.index),     # to match minority class\n",
    "                                     random_state=123) # reproducible results\n",
    "\n",
    "    # Combine minority class with downsampled majority class\n",
    "    df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "    # Display new class counts\n",
    "    print('downsampled\\n', df_downsampled['target'].value_counts())\n",
    "    \n",
    "    y_train = X_train['target']\n",
    "    X_train = X_train.drop(['target'], axis=1)\n",
    "    return X_train, y_train\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**read_data_small** is the function to read in the small dataset about 30 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_small(max_rows):\n",
    "    X_train = pd.read_csv(\"data_small/X_train_small.csv\")\n",
    "    X_test = pd.read_csv(\"data_small/X_test_small.csv\")\n",
    "    y_train = np.asarray(pd.read_csv(\"data_small/y_train_small.csv\", header=None)[0])\n",
    "    if max_rows:\n",
    "        # Take random subset of rows\n",
    "        \n",
    "        rows = np.random.choice(np.arange(len(X_train)), max_rows, False)\n",
    "        print(X_train.shape, X_test.shape, len(y_train))\n",
    "        print('random rows', rows)\n",
    "        return X_train.iloc[rows], X_test, y_train[rows]\n",
    "#         return X_train[:max_rows], X_test[:max_rows], y_train[:max_rows]\n",
    "    return X_train, X_test, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**read_data_big** is the function to read in the big dataset about 100 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_big(max_rows):\n",
    "    X_train = pd.read_csv(\"data_big/X_train_big.csv\")\n",
    "    X_test = pd.read_csv(\"data_big/X_test_big.csv\")\n",
    "    y_train = np.asarray(pd.read_csv(\"data_big/y_train_big.csv\", header=None)[0])\n",
    "    return X_train, X_test, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**read_data** is the function to read in the whole dataset about 1.5 G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(max_rows):\n",
    "    X_train = pd.read_csv(\"data/X_train.csv\")\n",
    "    X_test = pd.read_csv(\"data/X_test.csv\")\n",
    "    y_train = np.asarray(pd.read_csv(\"data/y_train.csv\", header=None)[0])\n",
    "    return X_train, X_test, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_wrapper(max_rows=None):\n",
    "    # return one of: read_data, read_data_big, read_data_wrapper\n",
    "    return read_data_small(max_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert Your Code Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**detect_spoofying** is the function for training the classifier and classify the results. \n",
    "\n",
    "Here we provide an simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141822, 29) (60782, 29) 141822\n",
      "random rows [ 30742  13950   4019  60276 121610 101992  34501  75884 131195  83371\n",
      "  80392  56039  87502  49109 126535 105472  45252  85672 112422  32927\n",
      "  97761  81936  97688 129086  56829  32948 111803  17122  56042  63377\n",
      "  70098 131940 106708  30555  73760 131843   7718  29475  30253  92238\n",
      " 118002  65693  35476  11591  24290  88348  42779  53921  75234   8986\n",
      "  60576  45670 141697   3573 108872  22966  17073  55690  24467  15185\n",
      " 140381 137675  48308 139105 127423  14993    572  16254  17657  41308\n",
      "  26589 112390  49006 124215  97727 125989    900  32043  38232  76117\n",
      "  36221  62264  35456  69869 111714 106403   6082  91257  52869   4316\n",
      "  67762  14719 109817 115960  47177  64455  40963  60121  51211  75809\n",
      " 104288  24961 120320 138896  16137 120244  49385 128699  65187  64020\n",
      "  66685 117647  13788  79176  74582   5813  67516   2125  45656  63626\n",
      "   3014  79493 113003 135317 114718 122587  94966  30549 128457  44346\n",
      "  12075 102748  24048  82946   8497 106388 141529   2449 111534  73039\n",
      "    145   5295   9868 137677  41836 136684  12328 114173  30797  90878\n",
      "  41776  49580  62471  99114  82350  60000  39241 110679 102626  82450\n",
      " 119532 102772  78055  79173  12382  26867  93316  74205  87023  80796\n",
      "  50629  75619  24517  77904  46038  98160 122663  56327 124298  39860\n",
      "  36093 129748   9807 109234 128872  45432  81894  79083  29280  37953\n",
      "   1395 131345  20607 105820  80941  21738  38749  64043  59571  63999\n",
      "  51215 127460  33153  80892  97696  73837  41842 109497  41502  62251\n",
      "  94941 103735  66501  66410  82679  33598   1182 139771  61821  68350\n",
      " 107124 137042  57252   8499 104863 122615  24646  47906 111779 124926\n",
      "  16072 134561  51594  51450 109121   5776   8199 105166  20829 105988\n",
      "  24113  72914 133259  97749  70033 109152  76251 132078  30487 109484\n",
      "  33587  99910 130828  53408 113563 103939  53141  23019 139952  17278\n",
      "  56968  66968  29869  37089 139622  48630  22842  37699 100009   3686\n",
      "  89244  73614  45949  94604 137511   9510  64985  38642 124439 110117\n",
      "  84508   8617  98471  34890 130645 131497  36888  15126  69327  17614\n",
      " 118020    171  37529  88579 123414 101245  52059  74119  53057  39763\n",
      "  45398   3963   4219  89765  81311 103960 125397  51770  27942  63840\n",
      "  72292  54851   2930  71886  54521  70108  93080  13083  75915   6635\n",
      "  32494 135790 134822 101238  28655 118942  61970 141574 113085  79901\n",
      " 108914 110957  11357     93 126427  79999  65921  45957  56986  31401\n",
      "  57546 104848 120459 140515  16065  89230 112429  47843   5459  79736\n",
      " 104624 122235 110779  43242  85680  69637  63722   7941  79577 138784\n",
      " 140885  56572  68610 140696  95835  88053  76732  71747  30508  89365\n",
      "  67783  38737  92466  68906 123771 102310 117059  54055  43970 134575\n",
      "  11462  87196  35163  38878  53787  59355 119286 110921 112156  14024\n",
      " 125777 134374 117360  69379 130080  80858  18289  97731  44648 110265\n",
      "  57445  55032 114426   2411  27641  20774  47558  57513  62213 122914\n",
      "  87544  67610  18488  51601  87118  31764  29233  32098  46375  51061\n",
      " 109975   2263  72843   8097  93888  25559 100223   2148  40935  71218\n",
      "  66318 116705  15720    772  29636  29220 109481  12657 133356  19110\n",
      "  38091   2100  87757 102670  94682  38760  47322  21144  37002   7554\n",
      "  74291  60351   5165   9577   7682  73210  86288  79452  75853  14911\n",
      "  92379 116944   8804  14400  27147  71966  99494  96007  85362  90589\n",
      "  76022  23110  34777 118513 126870  76560 107252   1096  80349 127533\n",
      "  92705  23474  10551 110067  27047  52529  39431 109366  82684 132476\n",
      "  30582 131174  65193  44794  35037  13937  55877  53466  43936  22540]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 500 entries, 30742 to 22540\n",
      "Data columns (total 29 columns):\n",
      "timestamp        500 non-null int64\n",
      "type             500 non-null object\n",
      "obId             500 non-null object\n",
      "member           500 non-null object\n",
      "user             500 non-null object\n",
      "endUserRef       500 non-null object\n",
      "price            500 non-null float64\n",
      "volume           500 non-null float64\n",
      "operation        490 non-null object\n",
      "isBid            490 non-null object\n",
      "orderId          490 non-null object\n",
      "source           490 non-null object\n",
      "tradeId          10 non-null object\n",
      "bidOrderId       10 non-null object\n",
      "askOrderId       10 non-null object\n",
      "isBuyer          10 non-null object\n",
      "isAggressor      10 non-null object\n",
      "bestBid          500 non-null float64\n",
      "bestBidVolume    500 non-null float64\n",
      "bestAsk          500 non-null float64\n",
      "bestAskVolume    500 non-null float64\n",
      "lv2Bid           500 non-null float64\n",
      "lv2BidVolume     500 non-null float64\n",
      "lv2Ask           500 non-null float64\n",
      "lv2AskVolume     500 non-null float64\n",
      "lv3Bid           500 non-null float64\n",
      "lv3BidVolume     500 non-null float64\n",
      "lv3Ask           500 non-null float64\n",
      "lv3AskVolume     500 non-null float64\n",
      "dtypes: float64(14), int64(1), object(14)\n",
      "memory usage: 117.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train = read_data_wrapper(MAX_ROWS)\n",
    "print(X_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "# X_train.info()\n",
    "print(len(set(X_train['user'])))\n",
    "# set(X_train['type'])\n",
    "print(len(set(X_train['member'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLS = [\"price\",\"volume\",\"bestBid\",\"bestAsk\",'bestBidVolume',\n",
    "                    'bestAskVolume','lv2Bid', 'lv2BidVolume','lv2Ask', \n",
    "                    'lv2AskVolume', 'lv3Bid', 'lv3BidVolume', 'lv3Ask',\n",
    "                    'lv3AskVolume']\n",
    "ENCODED_COLS = ['member', 'user']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import libraries here ###\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "### code classifier here ###\n",
    "# Feature engineering\n",
    "def format_data(df):\n",
    "\n",
    "    cols = []\n",
    "    cols.extend(NUMERIC_COLS)\n",
    "    cols.extend(ENCODED_COLS)\n",
    "    \n",
    "    # append numberical columns\n",
    "    rst = df.loc[:, cols]\n",
    "    \n",
    "    # encode the binaries\n",
    "    rst[\"isBid\"] = df.isBid*1\n",
    "    rst[\"isBuyer\"] = df.isBuyer*1\n",
    "    rst[\"isAggressor\"] = df.isAggressor*1\n",
    "    rst[\"type\"] = (df.type == \"ORDER\")*1\n",
    "    rst[\"source\"] = (df.source==\"USER\")*1\n",
    "\n",
    "    # parse the order id data\n",
    "    rst[\"orderId\"] = df.orderId.str.split('-').str[-1]\n",
    "    rst[\"tradeId\"] = df.tradeId.str.split('-').str[-1]\n",
    "    rst[\"bidOrderId\"] = df.bidOrderId.str.split('-').str[-1]\n",
    "    rst[\"askOrderId\"] = df.askOrderId.str.split('-').str[-1]\n",
    "    \n",
    "    \n",
    "    # encode the multiple label data\n",
    "    tmp_operation = pd.DataFrame(pd.get_dummies(df.operation, prefix=\"op\"), columns=df.operation.unique()[:-1])\n",
    "    rst = pd.concat([rst, tmp_operation], axis=1)\n",
    "    tmp_endUserRef = pd.DataFrame(pd.get_dummies(df.endUserRef, prefix=\"enduser\"), columns=df.endUserRef.unique()[:-1])\n",
    "    rst = pd.concat([rst, tmp_endUserRef], axis=1)\n",
    "    \n",
    "    # also feel free to add more columns inferred from data\n",
    "    # smartly engineered features can be very useful to improve the classification results\n",
    "    rst[\"timeSinceLastTrade\"] = X_train[[\"timestamp\",\"endUserRef\"]].groupby(\"endUserRef\").diff()\n",
    "\n",
    "    for col in ENCODED_COLS:\n",
    "        # https://stackoverflow.com/questions/37292872/how-can-i-one-hot-encode-in-python\n",
    "        # one hot encode\n",
    "        # Get one hot encoding of columns B\n",
    "        one_hot = pd.get_dummies(df[col], prefix=col)\n",
    "        # Drop column B as it is now encoded\n",
    "        rst = rst.drop(col, axis = 1)\n",
    "        # Join the encoded df\n",
    "        rst = rst.join(one_hot)\n",
    "    print('data shape', rst.shape)\n",
    "    print('cols', len(rst.columns.values))\n",
    "    return rst\n",
    "\n",
    "def get_scaled_data(X_train, X_test, y_train):\n",
    "    \n",
    "    X_train, y_train = downsample(X_train, y_train)\n",
    "        \n",
    "    # clean up the data\n",
    "#     df = df[~df.index.duplicated()]\n",
    "    X_clean = format_data(pd.concat([X_train, X_test]))\n",
    "    for c in NUMERIC_COLS:\n",
    "        X_clean[c]= X_clean[c].fillna(X_clean[c].mean())\n",
    "#     X_clean.fillna(method='ffill', inplace=True)\n",
    "    X_clean = X_clean.fillna(-1)\n",
    "    \n",
    "    feature_columns = X_clean.columns.values\n",
    "\n",
    "    X_train_clean = X_clean.iloc[:X_train.shape[0],:]\n",
    "    X_test_clean = X_clean.iloc[X_train.shape[0]:,:]\n",
    "    X_train_clean_scaled = scale(X_train_clean)\n",
    "    X_test_clean_scaled = scale(X_test_clean)\n",
    "    return X_train_clean_scaled, X_test_clean_scaled, y_train, feature_columns\n",
    "\n",
    "# Classification algorithm\n",
    "def detect_spoofying(X_train, X_test, y_train):\n",
    "    X_train_clean_scaled, X_test_clean_scaled, y_train, feature_columns = get_scaled_data(X_train, X_test, y_train)\n",
    "    \n",
    "#     clf = ExtraTreesClassifier() # .89\n",
    "#     clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(15), random_state=1)\n",
    "#     clf = LinearSVC(random_state=0, tol=1e-5, multi_class=\"crammer_singer\")\n",
    "#     clf = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "#     clf = OneVsRestClassifier(XGBClassifier())\n",
    "    clf = OneVsRestClassifier(XGBClassifier(n_estimators=100, n_jobs=-1, max_depth=4))\n",
    "#     clf = OneVsRestClassifier(XGBClassifier())\n",
    "    clf.fit(X_train_clean_scaled, y_train)\n",
    "    # print('features', list(zip(feature_columns, clf.feature_importances_)))\n",
    "    y_train_prob_pred = clf.predict_proba(X_train_clean_scaled)\n",
    "    y_test_prob_pred = clf.predict_proba(X_test_clean_scaled)\n",
    "    \n",
    "    return y_train_prob_pred, y_test_prob_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VISUALIZE:\n",
    "    X_train, X_test, y_train = read_data_wrapper(MAX_ROWS)\n",
    "    X_train_clean_scaled, X_test_clean_scaled, y_train, feature_columns = get_scaled_data(X_train, X_test, y_train)\n",
    "\n",
    "    clf = BalancedRandomForestClassifier(max_depth=2, random_state=0)\n",
    "\n",
    "    # fit classifier\n",
    "    #     clf = LogisticRegression(random_state=0, class_weight='balanced')\n",
    "    clf.fit(X_train_clean_scaled, y_train)\n",
    "    print('features', list(zip(feature_columns, clf.feature_importances_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VISUALIZE:\n",
    "    names = feature_columns\n",
    "    values = clf.feature_importances_\n",
    "    zipped = zip(names, values)\n",
    "    res = sorted(zipped, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    n = 50\n",
    "    names = [make_readable(i[0]) for i in res[:n]]\n",
    "    values = [i[1] for i in res[:n]]\n",
    "\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    sns.barplot(values, names)\n",
    "\n",
    "    plt.title('Feature Importance')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = X_train.loc[:, NUMERIC_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VISUALIZE:\n",
    "    X_num.describe()\n",
    "    # set(y_train)\n",
    "    y_categories = pd.get_dummies(pd.DataFrame(y_train).replace({0: '0', 1: '1', 2: '2'}),prefix='Category')\n",
    "    # print(y_categories.loc[y_categories['0_Category 1'] == 1])\n",
    "    corr_df = X_num.join(y_categories)\n",
    "    corr = corr_df.corr()\n",
    "    plt.figure(figsize = (16,9))\n",
    "    ax = sns.heatmap(corr, annot=True, linewidths=.5, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**score** is the function that we use to compare the results. An example is provided with scoring the predictions for the training dataset. True labels for the testing data set will be supplied to score the predictions for testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score is based on cohen's kappa measurement. https://en.wikipedia.org/wiki/Cohen%27s_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def score(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    y_pred: a numpy 4d array of probabilities of point assigned to each label\n",
    "    y_true: a numpy array of true labels\n",
    "    \"\"\"\n",
    "    y_pred_label = np.argmax(y_pred, axis=1)\n",
    "    return cohen_kappa_score(y_pred_label, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional: examples of k-fold cross validation ###\n",
    "# k-fold cross validation can help you compare the classification models\n",
    "if False:\n",
    "    from sklearn.model_selection import KFold\n",
    "    n = 5\n",
    "    kf = KFold(n_splits = n)\n",
    "    X_train, X_test, y_train = read_data_wrapper(MAX_ROWS)\n",
    "    kf.get_n_splits(X_train)\n",
    "    print(kf)\n",
    "    kf_scores = pd.DataFrame(np.zeros([n,2]), columns=[\"train score\", \"test score\"])\n",
    "    rowindex = 0\n",
    "    i = 0\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        i+=1\n",
    "        print('Step', i, \"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        print(X_train.index)\n",
    "        print(y_train)\n",
    "        X_train_kf, X_test_kf = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_kf, y_test_kf = y_train[train_index], y_train[test_index]\n",
    "        y_train_prob_pred_kf, y_test_prob_pred_kf = detect_spoofying(X_train_kf, X_test_kf, y_train_kf)\n",
    "        score_train_kf = score(y_train_prob_pred_kf, y_train_kf)\n",
    "        score_test_kf = score(y_test_prob_pred_kf, y_test_kf)\n",
    "        kf_scores.iloc[rowindex, 0] = score_train_kf\n",
    "        kf_scores.iloc[rowindex, 1] = score_test_kf\n",
    "        rowindex += 1\n",
    "    \n",
    "    print('scores')\n",
    "    print(kf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**wrapper** is the main function to read in unzipped data and output a score for evaluation. In addition, the function returns the y probability matrix (both train and test) for grading. More details about submitting format are outlined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper():\n",
    "    # read in data\n",
    "    X_train, X_test, y_train = read_data_wrapper(MAX_ROWS)\n",
    "    # or if you have the computational power to work with the big data set, \n",
    "    # you can comment out the read_data_samll line and uncomment the following read_data_big\n",
    "    # X_train, X_test, y_train = read_data_big()\n",
    "    \n",
    "    # process the data, train classifier and output probability matrix\n",
    "    y_train_prob_pred, y_test_prob_pred = detect_spoofying(X_train, X_test, y_train)\n",
    "    \n",
    "    # score the predictions\n",
    "    print(len(y_train_prob_pred), len(y_train))\n",
    "    score_train = score(y_train_prob_pred, y_train)\n",
    "    # score_test = score(y_test_prob_pred, y_test)\n",
    "    \n",
    "    # return the scores\n",
    "    return score_train, y_train_prob_pred, y_test_prob_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call function wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(141822, 29) (60782, 29) 141822\n",
      "random rows [ 47971  84953  55118  11019  71960  81359  93332  96241  67779  49605\n",
      " 132439  12923 100038   3369  24537  85111  92593  45649  21637 141736\n",
      "   2167   7123 115607  55482  62787  19124    431  10241  46785  87374\n",
      "  32864   8656  89793  79317  62187  11688 136400  77509 104323  41320\n",
      "  51249 116799  18063  37749  46594 132115 105775  40278  16154 136289\n",
      "  57409  88367    685  39781  84065 116133 140808  24104 139225  70304\n",
      "  92629 131151 124814 110514 136722  56903  83178  53044  37473  38742\n",
      "  97967  39680   3847 119639 127708  20394  38505 105095  64852 100036\n",
      "     17  71205  89670  71426 126039  28514  90980  73905   3242  33239\n",
      "  22047  60154  78453  96492  27538  70197 115142  92006 132221 119411\n",
      "  29874  31310 110536   1975  38012  40917  96356  83230  75388 131626\n",
      " 100994 126834 111948 131283 109164 100264  46907   7289 122824  17218\n",
      " 109456  46077  65077  19692  88221   5950 112311  37168  98712 107534\n",
      " 140607  47450 119656 136583 105415  17671  33080  17239  17690  72090\n",
      "   4131 116925   2026 141046 100117  74475  28347  18419  55856 120169\n",
      " 110404  42295 119254 120896  35636  51517  43574  81831  63758  69782\n",
      " 102302  86808  84408  64375  34457  84935 128214 114474 110642  80302\n",
      "  92380  70519 126292  71802 136629 127180 114299 132208  92708  65818\n",
      "  43713 132302  54640  72069  63839  53098   2640  78126  28759  86594\n",
      "  12319 116935 140713 126229  45129  14194 103227  72819 106529   6601\n",
      " 112743 108947  96295   1899  40483  22300  80709 103537  88572  99740\n",
      "   6676  10603  89472 134013  92529  27786   9651  23999 112376  87608\n",
      "  70079  39660  20768 113845  98530 121235  73019  62818 102800  90846\n",
      "  14404 118631   5472  57851  79357  69689 122089  25913  74451 133482\n",
      " 110550 118030   8505   7626 121619  17358  21456  19574  52413  70524\n",
      "  84243  82773  53396  80232  88553  31316 128427  24525  30217 126210\n",
      " 119004 133158  11025  11514  57526  55218 131903 132437  96041  50903\n",
      "   1825 132948  56482  28791  42080  87209  73313  92044  81595  73184\n",
      "  60008  27381  38726  59027  42717  88821  64059 115682  81452  55729\n",
      "  75558 113381  73372   4951 140762 104970 105302  92049  58390  10938\n",
      "  20414  23693  10301  58862 133415  39925  41238 112986  49104 109866\n",
      "  91625 107882  12298  50694   8293 133166 115893  40835 132945   7615\n",
      "  87586 114581  25558   3792  43283 108241  97523  20758 104023  92736\n",
      "  36435 116298 133447 132862  10577   2697  19899  23949  93732  12439\n",
      "  11066  52181  82275 137174  97762 127150 126833 140267  26196  41935\n",
      "  24265 132714  63595  13340 128476   8903  70497  78960  38420  87668\n",
      " 132519 126618  86796 101768  85961  51716  59483  58022  81729  34312\n",
      "   2604  33773  44181  82942  79708  84878  30817 107415  99301 108799\n",
      "  99658  27918  10753  21787 135941 119156 126367  53176  62778  17126\n",
      "  60532   1717  61192 113299  74199  58602 111777  73504 104228  68558\n",
      " 130489  87936  69604  50017  69327  64880  45642 115338 110532  39153\n",
      "  63751  50255 106879 128358   3787 120270  91330  20432  83521   3333\n",
      " 100165  33652  34871   7422  70107  66831 136884  44295 100313  13995\n",
      "  72997  47639  12619  79912  66351 141452  19012  77388  82250  94995\n",
      "  79351  77535  47935 126608  15392 138211  57453 113243  47328 141453\n",
      "  36164  48595    195 133125 138444   9487  11712     28  71035  11355\n",
      "  16707  44159  19618  80723   6511   9949  37550 139874  32850  88640\n",
      "  13240  39265   9212 134583 140010    927  15323  46310   3351 138033\n",
      " 110785  78966 140546 139393  56335  81462  94123 124055  98445  65335\n",
      " 108477  77312  78460  10001  21748 133104  40718   3258   5699  11057]\n",
      "data shape (62578, 782)\n",
      "cols 782\n",
      "500 500\n"
     ]
    }
   ],
   "source": [
    "score_train, y_train_prob_pred, y_test_prob_pred = wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score for training data set is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Format\n",
    "\n",
    "The classifier function wrote should return a 4d nparray with 4 columns. The columns are corresponding to the class labels: 0, 1, 2, 3. Please see examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9986099 , 0.00139008],\n",
       "       [0.99754244, 0.00245753],\n",
       "       [0.99865454, 0.00134548],\n",
       "       [0.99453163, 0.00546836],\n",
       "       [0.99863803, 0.00136199],\n",
       "       [0.99748415, 0.00251587],\n",
       "       [0.99296457, 0.00703546],\n",
       "       [0.99865454, 0.00134548],\n",
       "       [0.99830765, 0.00169234],\n",
       "       [0.99755204, 0.00244794],\n",
       "       [0.99681044, 0.00318958],\n",
       "       [0.99755204, 0.00244794],\n",
       "       [0.99687475, 0.00312523],\n",
       "       [0.99795175, 0.00204827],\n",
       "       [0.99830765, 0.00169234],\n",
       "       [0.99127007, 0.00872991],\n",
       "       [0.99830765, 0.00169234],\n",
       "       [0.9975921 , 0.00240791],\n",
       "       [0.9975921 , 0.00240791],\n",
       "       [0.9975921 , 0.00240791],\n",
       "       [0.9975921 , 0.00240791],\n",
       "       [0.9982728 , 0.00172724],\n",
       "       [0.9982728 , 0.00172724],\n",
       "       [0.9982728 , 0.00172724],\n",
       "       [0.9982728 , 0.00172724],\n",
       "       [0.9953894 , 0.00461058],\n",
       "       [0.9969126 , 0.00308743],\n",
       "       [0.9969126 , 0.00308743],\n",
       "       [0.99795175, 0.00204827],\n",
       "       [0.99049747, 0.00950255],\n",
       "       [0.9982728 , 0.00172724],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.9975016 , 0.00249838],\n",
       "       [0.99755204, 0.00244794],\n",
       "       [0.99687475, 0.00312523],\n",
       "       [0.99678314, 0.00321684],\n",
       "       [0.99799514, 0.00200485],\n",
       "       [0.9973064 , 0.00269357],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.9561474 , 0.04385262],\n",
       "       [0.97793794, 0.02206208],\n",
       "       [0.9727609 , 0.02723906],\n",
       "       [0.9864134 , 0.01358656],\n",
       "       [0.97580916, 0.02419085],\n",
       "       [0.9975016 , 0.00249838],\n",
       "       [0.9982728 , 0.00172724],\n",
       "       [0.99681044, 0.00318958],\n",
       "       [0.9982728 , 0.00172724],\n",
       "       [0.9973078 , 0.0026922 ],\n",
       "       [0.9762871 , 0.02371287],\n",
       "       [0.99755204, 0.00244794],\n",
       "       [0.99687475, 0.00312523],\n",
       "       [0.99127007, 0.00872991],\n",
       "       [0.99795175, 0.00204827],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.97580916, 0.02419085],\n",
       "       [0.99832815, 0.00167184],\n",
       "       [0.9961319 , 0.00386813],\n",
       "       [0.9962557 , 0.00374432],\n",
       "       [0.9975154 , 0.00248462],\n",
       "       [0.98920935, 0.01079066],\n",
       "       [0.9917743 , 0.0082257 ],\n",
       "       [0.997947  , 0.00205303],\n",
       "       [0.99620986, 0.00379014],\n",
       "       [0.9626792 , 0.03732078],\n",
       "       [0.9975154 , 0.00248462],\n",
       "       [0.92914754, 0.07085244],\n",
       "       [0.9970308 , 0.00296918],\n",
       "       [0.9961786 , 0.00382137],\n",
       "       [0.99452275, 0.00547722],\n",
       "       [0.99620986, 0.00379014],\n",
       "       [0.99797183, 0.00202816],\n",
       "       [0.9970308 , 0.00296918],\n",
       "       [0.99836767, 0.00163236],\n",
       "       [0.99836767, 0.00163236],\n",
       "       [0.9969485 , 0.00305153],\n",
       "       [0.99834764, 0.00165238],\n",
       "       [0.9986271 , 0.0013729 ],\n",
       "       [0.9974117 , 0.00258826],\n",
       "       [0.9634253 , 0.03657469],\n",
       "       [0.9983382 , 0.00166176],\n",
       "       [0.9986271 , 0.0013729 ],\n",
       "       [0.997464  , 0.00253601],\n",
       "       [0.9983382 , 0.00166176],\n",
       "       [0.997464  , 0.00253601],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.9983382 , 0.00166176],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.9986271 , 0.0013729 ],\n",
       "       [0.9983382 , 0.00166176],\n",
       "       [0.9963331 , 0.00366691],\n",
       "       [0.99276304, 0.00723694],\n",
       "       [0.9986271 , 0.0013729 ],\n",
       "       [0.9979729 , 0.00202711],\n",
       "       [0.99859875, 0.00140122],\n",
       "       [0.9974431 , 0.00255693],\n",
       "       [0.9983382 , 0.00166176],\n",
       "       [0.997464  , 0.00253601],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.9974431 , 0.00255693],\n",
       "       [0.9974431 , 0.00255693],\n",
       "       [0.99864376, 0.00135627],\n",
       "       [0.9974431 , 0.00255693],\n",
       "       [0.99801385, 0.00198616],\n",
       "       [0.9986271 , 0.0013729 ],\n",
       "       [0.99835455, 0.00164544],\n",
       "       [0.99555343, 0.00444654],\n",
       "       [0.995462  , 0.00453798],\n",
       "       [0.99686074, 0.00313928],\n",
       "       [0.99798405, 0.00201593],\n",
       "       [0.99832064, 0.00167937],\n",
       "       [0.9968987 , 0.00310131],\n",
       "       [0.9968987 , 0.00310131],\n",
       "       [0.9969613 , 0.00303873],\n",
       "       [0.99798405, 0.00201593],\n",
       "       [0.9983344 , 0.00166562],\n",
       "       [0.99798405, 0.00201593],\n",
       "       [0.99834234, 0.00165766],\n",
       "       [0.99834234, 0.00165766],\n",
       "       [0.99834234, 0.00165766],\n",
       "       [0.9979691 , 0.0020309 ],\n",
       "       [0.99834234, 0.00165766],\n",
       "       [0.99682283, 0.00317715],\n",
       "       [0.99682283, 0.00317715],\n",
       "       [0.9979691 , 0.0020309 ],\n",
       "       [0.998322  , 0.00167799],\n",
       "       [0.996784  , 0.00321605],\n",
       "       [0.99682283, 0.00317715],\n",
       "       [0.9979691 , 0.0020309 ],\n",
       "       [0.99834234, 0.00165766],\n",
       "       [0.99834234, 0.00165766],\n",
       "       [0.99834234, 0.00165766],\n",
       "       [0.9979691 , 0.0020309 ],\n",
       "       [0.998322  , 0.00167799],\n",
       "       [0.996784  , 0.00321605],\n",
       "       [0.99682283, 0.00317715],\n",
       "       [0.9979691 , 0.0020309 ],\n",
       "       [0.998322  , 0.00167799],\n",
       "       [0.996784  , 0.00321605],\n",
       "       [0.99682283, 0.00317715],\n",
       "       [0.9979691 , 0.0020309 ],\n",
       "       [0.99834234, 0.00165766],\n",
       "       [0.99834234, 0.00165766],\n",
       "       [0.99834234, 0.00165766],\n",
       "       [0.9979691 , 0.0020309 ],\n",
       "       [0.99757284, 0.00242719],\n",
       "       [0.9953512 , 0.00464879],\n",
       "       [0.99682283, 0.00317715],\n",
       "       [0.9973845 , 0.00261549],\n",
       "       [0.9979404 , 0.00205958],\n",
       "       [0.9979404 , 0.00205958],\n",
       "       [0.99791515, 0.00208483],\n",
       "       [0.9967511 , 0.0032489 ],\n",
       "       [0.99759066, 0.00240931],\n",
       "       [0.9974038 , 0.00259623],\n",
       "       [0.99692404, 0.00307594],\n",
       "       [0.9969613 , 0.00303873],\n",
       "       [0.9974947 , 0.00250531],\n",
       "       [0.99859875, 0.00140122],\n",
       "       [0.9983382 , 0.00166176],\n",
       "       [0.99276304, 0.00723694],\n",
       "       [0.99861574, 0.00138424],\n",
       "       [0.9974431 , 0.00255693],\n",
       "       [0.9974947 , 0.00250531],\n",
       "       [0.99686676, 0.00313322],\n",
       "       [0.99861574, 0.00138424],\n",
       "       [0.99785966, 0.00214032],\n",
       "       [0.99411917, 0.00588081],\n",
       "       [0.99785966, 0.00214032],\n",
       "       [0.99864376, 0.00135627],\n",
       "       [0.99437475, 0.00562522],\n",
       "       [0.997464  , 0.00253601],\n",
       "       [0.99859875, 0.00140122],\n",
       "       [0.997464  , 0.00253601],\n",
       "       [0.9974431 , 0.00255693],\n",
       "       [0.9986271 , 0.0013729 ],\n",
       "       [0.99859875, 0.00140122],\n",
       "       [0.9974431 , 0.00255693],\n",
       "       [0.9983382 , 0.00166176],\n",
       "       [0.998322  , 0.00167799],\n",
       "       [0.998322  , 0.00167799],\n",
       "       [0.99834234, 0.00165766],\n",
       "       [0.9979691 , 0.0020309 ],\n",
       "       [0.99834234, 0.00165766],\n",
       "       [0.99834234, 0.00165766],\n",
       "       [0.99834234, 0.00165766],\n",
       "       [0.9979691 , 0.0020309 ],\n",
       "       [0.99834234, 0.00165766],\n",
       "       [0.99682283, 0.00317715],\n",
       "       [0.99682283, 0.00317715],\n",
       "       [0.9979691 , 0.0020309 ],\n",
       "       [0.998322  , 0.00167799],\n",
       "       [0.998322  , 0.00167799],\n",
       "       [0.99834234, 0.00165766],\n",
       "       [0.9979691 , 0.0020309 ],\n",
       "       [0.99757284, 0.00242719],\n",
       "       [0.99757284, 0.00242719],\n",
       "       [0.9907664 , 0.00923361],\n",
       "       [0.9979691 , 0.0020309 ],\n",
       "       [0.998322  , 0.00167799],\n",
       "       [0.998322  , 0.00167799],\n",
       "       [0.9973845 , 0.00261549],\n",
       "       [0.9973845 , 0.00261549],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.9974117 , 0.00258826],\n",
       "       [0.9974947 , 0.00250531],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.99859875, 0.00140122],\n",
       "       [0.9983382 , 0.00166176],\n",
       "       [0.99754244, 0.00245753],\n",
       "       [0.99605787, 0.00394211],\n",
       "       [0.99706495, 0.00293508],\n",
       "       [0.9952932 , 0.00470677],\n",
       "       [0.99861574, 0.00138424],\n",
       "       [0.99785924, 0.00214078],\n",
       "       [0.99834645, 0.00165357],\n",
       "       [0.9974431 , 0.00255693],\n",
       "       [0.9974947 , 0.00250531],\n",
       "       [0.9936571 , 0.00634291],\n",
       "       [0.99864376, 0.00135627],\n",
       "       [0.9979729 , 0.00202711],\n",
       "       [0.99686676, 0.00313322],\n",
       "       [0.99754244, 0.00245753],\n",
       "       [0.99801385, 0.00198616],\n",
       "       [0.99859875, 0.00140122],\n",
       "       [0.9974947 , 0.00250531],\n",
       "       [0.99864376, 0.00135627],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.99859875, 0.00140122],\n",
       "       [0.99269736, 0.00730266],\n",
       "       [0.99746335, 0.00253663],\n",
       "       [0.99282026, 0.00717974],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.9924946 , 0.00750545],\n",
       "       [0.99835145, 0.00164855],\n",
       "       [0.9980296 , 0.00197038],\n",
       "       [0.99801385, 0.00198616],\n",
       "       [0.99859875, 0.00140122],\n",
       "       [0.9986271 , 0.0013729 ],\n",
       "       [0.9983382 , 0.00166176],\n",
       "       [0.9983382 , 0.00166176],\n",
       "       [0.9983382 , 0.00166176],\n",
       "       [0.99859875, 0.00140122],\n",
       "       [0.9974947 , 0.00250531],\n",
       "       [0.994488  , 0.005512  ],\n",
       "       [0.9983382 , 0.00166176],\n",
       "       [0.99785966, 0.00214032],\n",
       "       [0.99801385, 0.00198616],\n",
       "       [0.99801385, 0.00198616],\n",
       "       [0.99861574, 0.00138424],\n",
       "       [0.99864376, 0.00135627],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.9974947 , 0.00250531],\n",
       "       [0.997464  , 0.00253601],\n",
       "       [0.997464  , 0.00253601],\n",
       "       [0.9963331 , 0.00366691],\n",
       "       [0.99859875, 0.00140122],\n",
       "       [0.9983382 , 0.00166176],\n",
       "       [0.994488  , 0.005512  ],\n",
       "       [0.9963331 , 0.00366691],\n",
       "       [0.9974947 , 0.00250531],\n",
       "       [0.9986271 , 0.0013729 ],\n",
       "       [0.997464  , 0.00253601],\n",
       "       [0.964139  , 0.03586102],\n",
       "       [0.9963331 , 0.00366691],\n",
       "       [0.9974117 , 0.00258826],\n",
       "       [0.997464  , 0.00253601],\n",
       "       [0.9974117 , 0.00258826],\n",
       "       [0.9974431 , 0.00255693],\n",
       "       [0.99864376, 0.00135627],\n",
       "       [0.9974947 , 0.00250531],\n",
       "       [0.9986271 , 0.0013729 ],\n",
       "       [0.9974431 , 0.00255693],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.99861574, 0.00138424],\n",
       "       [0.99861574, 0.00138424],\n",
       "       [0.997464  , 0.00253601],\n",
       "       [0.9986271 , 0.0013729 ],\n",
       "       [0.99861574, 0.00138424],\n",
       "       [0.99830395, 0.00169602],\n",
       "       [0.9974117 , 0.00258826],\n",
       "       [0.9974947 , 0.00250531],\n",
       "       [0.9986271 , 0.0013729 ],\n",
       "       [0.9974431 , 0.00255693],\n",
       "       [0.99861574, 0.00138424],\n",
       "       [0.99801385, 0.00198616],\n",
       "       [0.9974431 , 0.00255693],\n",
       "       [0.99859875, 0.00140122],\n",
       "       [0.9986271 , 0.0013729 ],\n",
       "       [0.99861574, 0.00138424],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.99276304, 0.00723694],\n",
       "       [0.997464  , 0.00253601],\n",
       "       [0.9929085 , 0.00709151],\n",
       "       [0.99785966, 0.00214032],\n",
       "       [0.9974117 , 0.00258826],\n",
       "       [0.99861574, 0.00138424],\n",
       "       [0.99861574, 0.00138424],\n",
       "       [0.99861574, 0.00138424],\n",
       "       [0.997464  , 0.00253601],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.9986271 , 0.0013729 ],\n",
       "       [0.9983382 , 0.00166176],\n",
       "       [0.99411917, 0.00588081],\n",
       "       [0.9872026 , 0.01279742],\n",
       "       [0.9969613 , 0.00303873],\n",
       "       [0.9983344 , 0.00166562],\n",
       "       [0.9968987 , 0.00310131],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.9969613 , 0.00303873],\n",
       "       [0.99759066, 0.00240931],\n",
       "       [0.99832064, 0.00167937],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.99835455, 0.00164544],\n",
       "       [0.9983344 , 0.00166562],\n",
       "       [0.99754107, 0.00245896],\n",
       "       [0.99798405, 0.00201593],\n",
       "       [0.992726  , 0.00727395],\n",
       "       [0.9983344 , 0.00166562],\n",
       "       [0.9979425 , 0.00205749],\n",
       "       [0.99798405, 0.00201593],\n",
       "       [0.97665346, 0.02334652],\n",
       "       [0.9969613 , 0.00303873],\n",
       "       [0.99692404, 0.00307594],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.99140704, 0.00859299],\n",
       "       [0.996784  , 0.00321605],\n",
       "       [0.99828744, 0.00171259],\n",
       "       [0.99525565, 0.00474436],\n",
       "       [0.998322  , 0.00167799],\n",
       "       [0.99828744, 0.00171259],\n",
       "       [0.9973845 , 0.00261549],\n",
       "       [0.99525565, 0.00474436],\n",
       "       [0.99828744, 0.00171259],\n",
       "       [0.99834234, 0.00165766],\n",
       "       [0.998322  , 0.00167799],\n",
       "       [0.996172  , 0.00382797],\n",
       "       [0.99828744, 0.00171259],\n",
       "       [0.9983082 , 0.00169184],\n",
       "       [0.9983082 , 0.00169184],\n",
       "       [0.9930155 , 0.00698448],\n",
       "       [0.9959779 , 0.00402214],\n",
       "       [0.99592865, 0.00407134],\n",
       "       [0.9979006 , 0.00209937],\n",
       "       [0.98841476, 0.01158522],\n",
       "       [0.9974282 , 0.00257182],\n",
       "       [0.99785733, 0.00214265],\n",
       "       [0.9959779 , 0.00402214],\n",
       "       [0.98864657, 0.01135342],\n",
       "       [0.99589515, 0.00410488],\n",
       "       [0.9978749 , 0.00212511],\n",
       "       [0.9958449 , 0.0041551 ],\n",
       "       [0.9958449 , 0.0041551 ],\n",
       "       [0.99785733, 0.00214265],\n",
       "       [0.9909196 , 0.00908042],\n",
       "       [0.98864657, 0.01135342],\n",
       "       [0.99754244, 0.00245753],\n",
       "       [0.9693004 , 0.03069963],\n",
       "       [0.9973752 , 0.0026248 ],\n",
       "       [0.99592865, 0.00407134],\n",
       "       [0.942471  , 0.05752895],\n",
       "       [0.9939963 , 0.00600365],\n",
       "       [0.99785733, 0.00214265],\n",
       "       [0.99589515, 0.00410488],\n",
       "       [0.99589515, 0.00410488],\n",
       "       [0.99587786, 0.00412216],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.99739605, 0.00260396],\n",
       "       [0.9974486 , 0.0025514 ],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.99799514, 0.00200485],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.9979706 , 0.00202942],\n",
       "       [0.9940438 , 0.00595615],\n",
       "       [0.99587786, 0.00412216],\n",
       "       [0.99596095, 0.00403907],\n",
       "       [0.99587786, 0.00412216],\n",
       "       [0.9974486 , 0.0025514 ],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.99671465, 0.00328532],\n",
       "       [0.9979173 , 0.0020827 ],\n",
       "       [0.9978744 , 0.00212563],\n",
       "       [0.9978918 , 0.00210823],\n",
       "       [0.9974486 , 0.0025514 ],\n",
       "       [0.99784833, 0.00215168],\n",
       "       [0.9978918 , 0.00210823],\n",
       "       [0.99784833, 0.00215168],\n",
       "       [0.9912366 , 0.00876338],\n",
       "       [0.9978744 , 0.00212563],\n",
       "       [0.9940438 , 0.00595615],\n",
       "       [0.9978744 , 0.00212563],\n",
       "       [0.9910572 , 0.00894278],\n",
       "       [0.9978744 , 0.00212563],\n",
       "       [0.9902857 , 0.00971428],\n",
       "       [0.9959277 , 0.00407234],\n",
       "       [0.99784833, 0.00215168],\n",
       "       [0.9968883 , 0.00311173],\n",
       "       [0.9979173 , 0.0020827 ],\n",
       "       [0.9869957 , 0.01300427],\n",
       "       [0.99273026, 0.00726975],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.998322  , 0.00167799],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.9979691 , 0.0020309 ],\n",
       "       [0.99525565, 0.00474436],\n",
       "       [0.9907664 , 0.00923361],\n",
       "       [0.9979691 , 0.0020309 ],\n",
       "       [0.99671775, 0.00328227],\n",
       "       [0.99834234, 0.00165766],\n",
       "       [0.9975228 , 0.0024772 ],\n",
       "       [0.99828744, 0.00171259],\n",
       "       [0.9983082 , 0.00169184],\n",
       "       [0.996172  , 0.00382797],\n",
       "       [0.9979006 , 0.00209937],\n",
       "       [0.9909861 , 0.0090139 ],\n",
       "       [0.99692667, 0.00307333],\n",
       "       [0.9958449 , 0.0041551 ],\n",
       "       [0.99589515, 0.00410488],\n",
       "       [0.99662024, 0.00337977],\n",
       "       [0.99592865, 0.00407134],\n",
       "       [0.99671775, 0.00328227],\n",
       "       [0.9983082 , 0.00169184],\n",
       "       [0.9930155 , 0.00698448],\n",
       "       [0.9967438 , 0.00325623],\n",
       "       [0.9962024 , 0.00379761],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.99833536, 0.00166465],\n",
       "       [0.99108875, 0.00891127],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.9968095 , 0.00319053],\n",
       "       [0.9983555 , 0.00164448],\n",
       "       [0.99678314, 0.00321684],\n",
       "       [0.9967438 , 0.00325623],\n",
       "       [0.99833536, 0.00166465],\n",
       "       [0.9862617 , 0.01373826],\n",
       "       [0.9962024 , 0.00379761],\n",
       "       [0.99684805, 0.00315194],\n",
       "       [0.99684805, 0.00315194],\n",
       "       [0.99684805, 0.00315194],\n",
       "       [0.99684805, 0.00315194],\n",
       "       [0.9684152 , 0.03158477],\n",
       "       [0.9684152 , 0.03158477],\n",
       "       [0.9684152 , 0.03158477],\n",
       "       [0.9684152 , 0.03158477],\n",
       "       [0.99538803, 0.00461196],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.97531456, 0.02468543],\n",
       "       [0.9975921 , 0.00240791],\n",
       "       [0.9974053 , 0.00259472],\n",
       "       [0.9967438 , 0.00325623],\n",
       "       [0.99798524, 0.00201476],\n",
       "       [0.9968095 , 0.00319053],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.9983555 , 0.00164448],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.9967438 , 0.00325623],\n",
       "       [0.99678314, 0.00321684],\n",
       "       [0.99678314, 0.00321684],\n",
       "       [0.99833536, 0.00166465],\n",
       "       [0.9952932 , 0.00470677],\n",
       "       [0.9967438 , 0.00325623],\n",
       "       [0.9973518 , 0.00264818],\n",
       "       [0.9968095 , 0.00319053],\n",
       "       [0.9967438 , 0.00325623],\n",
       "       [0.99108875, 0.00891127],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.9929286 , 0.00707139],\n",
       "       [0.9983216 , 0.0016784 ],\n",
       "       [0.99798524, 0.00201476],\n",
       "       [0.9983555 , 0.00164448],\n",
       "       [0.9967438 , 0.00325623],\n",
       "       [0.9968095 , 0.00319053],\n",
       "       [0.9979425 , 0.00205749],\n",
       "       [0.99759066, 0.00240931],\n",
       "       [0.99680763, 0.00319238],\n",
       "       [0.9983    , 0.00169996],\n",
       "       [0.9983    , 0.00169996],\n",
       "       [0.9967419 , 0.00325811],\n",
       "       [0.998301  , 0.00169898],\n",
       "       [0.99538535, 0.00461463],\n",
       "       [0.99798405, 0.00201593],\n",
       "       [0.9983    , 0.00169996]], dtype=float32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_prob_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9939963 , 0.00600365],\n",
       "       [0.9978749 , 0.00212511],\n",
       "       [0.9959779 , 0.00402214],\n",
       "       ...,\n",
       "       [0.9974323 , 0.00256771],\n",
       "       [0.9974323 , 0.00256771],\n",
       "       [0.9982749 , 0.00172506]], dtype=float32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_prob_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_train_prob_pred).to_csv(\"y_train_prob_pred.csv\")\n",
    "pd.DataFrame(y_test_prob_pred).to_csv(\"y_test_prob_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
